{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT2uOt_qtWcv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "wIasjN1Ys76O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "M5kXxw-XcmD6",
        "outputId": "5e5d81be-558c-4ded-d9d2-8f79ee38f851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Sensor Data Shape: (1122772, 6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Read all acc_expXX_userYY.txt and gyro_expXX_userYY.txt files.\\nExtract X, Y, Z values from both accelerometer and gyroscope.\\nMerge them into a single dataset.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "\n",
        "RAW_DATA_PATH = \"\"  # Path where raw files are stored\n",
        "\n",
        "# Load all accelerometer and gyroscope files\n",
        "acc_files = sorted(glob.glob(os.path.join(RAW_DATA_PATH, \"acc_exp*.txt\")))\n",
        "gyro_files = sorted(glob.glob(os.path.join(RAW_DATA_PATH, \"gyro_exp*.txt\")))\n",
        "\n",
        "# Function to load sensor data (accelerometer or gyroscope)\n",
        "def load_sensor_data(file_list):\n",
        "    data = []\n",
        "    for file in file_list:\n",
        "        sensor_data = np.loadtxt(file)  # Load file as numpy array\n",
        "        data.append(sensor_data)\n",
        "    return np.vstack(data)  # Stack all experiments together\n",
        "\n",
        "# Load all sensor readings\n",
        "acc_data = load_sensor_data(acc_files)  # Shape (N, 3) â†’ X, Y, Z acceleration\n",
        "gyro_data = load_sensor_data(gyro_files)  # Shape (N, 3) â†’ X, Y, Z angular velocity\n",
        "\n",
        "# Combine accelerometer and gyroscope data (6 features per timestamp)\n",
        "X_raw = np.hstack((acc_data, gyro_data))  # Shape (N, 6)\n",
        "\n",
        "print(f\"Raw Sensor Data Shape: {X_raw.shape}\")  # Expect (Total_samples, 6)\n",
        "\n",
        "\n",
        "\"\"\" Read all acc_expXX_userYY.txt and gyro_expXX_userYY.txt files.\n",
        "Extract X, Y, Z values from both accelerometer and gyroscope.\n",
        "Merge them into a single dataset.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-VtXP8Mi30_",
        "outputId": "95528b83-b9cd-4161-f9dc-ae5388956389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.91805559, -0.1125    ,  0.50972225, -0.05497787, -0.06963864,\n",
              "       -0.03084869])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in labels.txt contains:<br>\n",
        "[experiment_id, user_id, activity_id, start_sample, end_sample]\n",
        "\n",
        "Example:\n",
        "\n",
        "1 1 5 0 128<br>\n",
        "1 1 2 128 256<br>\n",
        "1 1 3 256 384<br>\n",
        "\n",
        "This means:\n",
        "\n",
        "Activity 5 starts at sample 0 and ends at 128.<br>\n",
        "Activity 2 starts at sample 128 and ends at 256.<br>\n",
        "Activity 3 starts at sample 256 and ends at 384.<br>\n",
        "We will:<br>\n",
        "\n",
        "- Read labels.txt.<br>\n",
        "- Assign activity labels to corresponding samples."
      ],
      "metadata": {
        "id": "gkwkyfkbibTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load labels file\n",
        "labels_path = os.path.join(RAW_DATA_PATH, \"labels.txt\")\n",
        "labels_df = pd.read_csv(labels_path, sep=\" \", header=None, names=[\"experiment\", \"user\", \"activity\", \"start\", \"end\"])\n",
        "\n",
        "# Create an empty label array\n",
        "y_raw = np.zeros((X_raw.shape[0],))  # One label per timestamp\n",
        "\n",
        "# Assign labels to sensor readings\n",
        "for _, row in labels_df.iterrows():\n",
        "    y_raw[row[\"start\"]:row[\"end\"]] = row[\"activity\"]  # Assign activity ID\n",
        "\n",
        "print(f\"Labels Shape: {y_raw.shape}\")  # Expect (Total_samples,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KC3GnvWfolM",
        "outputId": "077b2d99-0350-4d60-c47b-44574a763ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels Shape: (1122772,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_raw[1400]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlIVcSn2flhb",
        "outputId": "5401326f-d00e-4871-aea2-3321d7dbcce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCkL0Nv-oONk",
        "outputId": "74982710-1bee-46ea-ce61-8749fae30a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Label Distribution in Validation Data: {0: 2751, 1: 4686, 2: 4742, 3: 3155, 4: 2785, 5: 2606, 6: 216, 7: 128, 8: 291, 9: 291, 10: 249, 11: 146, 12: 1100676}\n"
          ]
        }
      ],
      "source": [
        "unique, counts = np.unique(y_seq.argmax(axis=1), return_counts=True)\n",
        "print(\"ðŸ“Š Label Distribution in Validation Data:\", dict(zip(unique, counts)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI9wLUlLrhPD"
      },
      "source": [
        "#It seems like there is high class imablance where the transitional actions like lie_to_sit, stand_to_sit etc seems to have too low count while lie_to_stand has extreme high count. lets balance this and see if we can handle the overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Cl5rBP2dv93S",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "61a09b97-c69a-4200-9704-06ecbc123617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Collecting imbalanced-learn (from imblearn)\n",
            "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
            "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn)\n",
            "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: sklearn-compat, imbalanced-learn, imblearn\n",
            "Successfully installed imbalanced-learn-0.13.0 imblearn-0.0 sklearn-compat-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-6THsIUr_Q0",
        "outputId": "7b0fabc0-aa78-4f46-d7ef-354c7c546676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Class Counts: {0: 2751, 1: 4686, 2: 4742, 3: 3155, 4: 2785, 5: 2606, 6: 216, 7: 128, 8: 291, 9: 291, 10: 249, 11: 146, 12: 1100676}\n",
            "New Class Counts: {0: 2751, 1: 4686, 2: 4742, 3: 3155, 4: 2785, 5: 2606, 6: 3000, 7: 3000, 8: 3000, 9: 3000, 10: 3000, 11: 3000, 12: 1100676}\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Using Smote to do over sampling and udnersampling of data\"\"\"\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Convert one-hot labels to categorical format\n",
        "y_train_flat = y_seq.argmax(axis=1)\n",
        "\n",
        "# Check class distribution\n",
        "unique, counts = np.unique(y_train_flat, return_counts=True)\n",
        "class_counts = dict(zip(unique, counts))\n",
        "print(\"Original Class Counts:\", class_counts)\n",
        "\n",
        "# Define target samples for rare classes (only increasing the rare ones)\n",
        "desired_samples = {\n",
        "    6: 3000,  # LAYING\n",
        "    7: 3000,  # STAND_TO_SIT\n",
        "    8: 3000,  # SIT_TO_STAND\n",
        "    9: 3000,  # SIT_TO_LIE\n",
        "    10: 3000, # LIE_TO_SIT\n",
        "    11: 3000, # STAND_TO_LIE\n",
        "}\n",
        "\n",
        "# Apply SMOTE only to specified rare classes\n",
        "smote = SMOTE(sampling_strategy=desired_samples, random_state=42)\n",
        "\n",
        "# Flatten LSTM input for SMOTE (convert 3D -> 2D)\n",
        "X_train_flat = X_seq.reshape(X_seq.shape[0], -1)\n",
        "\n",
        "# Resample data\n",
        "X_resampled_flat, y_resampled_flat = smote.fit_resample(X_train_flat, y_train_flat)\n",
        "\n",
        "# Reshape back to seq format\n",
        "X_resampled = X_resampled_flat.reshape(-1, TIME_STEPS, FEATURES)\n",
        "y_resampled = to_categorical(y_resampled_flat, num_classes=y_seq.shape[1])\n",
        "\n",
        "# Print new class distribution\n",
        "new_counts = dict(zip(*np.unique(y_resampled.argmax(axis=1), return_counts=True)))\n",
        "print(\"New Class Counts:\", new_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01ysdpN7xAPS"
      },
      "source": [
        "#Now low count classes has been balanced. lets balance high count class 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXQRUuvpw_-q",
        "outputId": "94668e7e-ac7a-48e5-ed21-76cab886a9b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced Training Data Shape: (42725, 50, 6), (42725, 13)\n",
            "New Class Counts: {0: 2751, 1: 4686, 2: 4742, 3: 3155, 4: 2785, 5: 2606, 6: 3000, 7: 3000, 8: 3000, 9: 3000, 10: 3000, 11: 3000, 12: 4000}\n"
          ]
        }
      ],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Define undersampling strategy (reduce to 5000 samples max per class)\n",
        "undersample = RandomUnderSampler(sampling_strategy={12: 4000}, random_state=42)\n",
        "\n",
        "# Apply undersampling\n",
        "X_under_flat, y_under_flat = undersample.fit_resample(X_resampled_flat, y_resampled_flat)\n",
        "\n",
        "# Reshape back\n",
        "X_under = X_under_flat.reshape(-1, TIME_STEPS, FEATURES)\n",
        "y_under = to_categorical(y_under_flat, num_classes=y_seq.shape[1])\n",
        "\n",
        "print(f\"Reduced Training Data Shape: {X_under.shape}, {y_under.shape}\")\n",
        "new_counts = dict(zip(*np.unique(y_under.argmax(axis=1), return_counts=True)))\n",
        "print(\"New Class Counts:\", new_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkzGbfsGyd9g"
      },
      "outputs": [],
      "source": [
        "# let us save the sampled balanced dataset for future purposes\n",
        "np.savez(\"Xraw_yraw_balanced.npz\",array1=X_under_flat, array2=y_under_flat)\n",
        "#when loading this npz file, we need to convert it into sequence format to retain the shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eRFMTNI_LP5",
        "outputId": "19c68e1a-b759-4b29-baec-d64648ad9d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced Training Data Shape: (42725, 50, 6), (42725, 13)\n",
            "New Class Counts: {0: 2751, 1: 4686, 2: 4742, 3: 3155, 4: 2785, 5: 2606, 6: 3000, 7: 3000, 8: 3000, 9: 3000, 10: 3000, 11: 3000, 12: 4000}\n"
          ]
        }
      ],
      "source": [
        "data = np.load(\"Xraw_yraw_balanced.npz\")\n",
        "X_under_flat= data[\"array1\"]\n",
        "y_under_flat= data[\"array2\"]\n",
        "\n",
        "TIME_STEPS = 50  # 1 second of data (50Hz sampling rate)\n",
        "FEATURES = 6 # 6 features = acc + gyro\n",
        "\n",
        "X_under = X_under_flat.reshape(-1, TIME_STEPS, FEATURES)\n",
        "y_under = to_categorical(y_under_flat, num_classes=13)\n",
        "\n",
        "print(f\"Reduced Training Data Shape: {X_under.shape}, {y_under.shape}\")\n",
        "new_counts = dict(zip(*np.unique(y_under.argmax(axis=1), return_counts=True)))\n",
        "print(\"New Class Counts:\", new_counts)"
      ]
    }
  ]
}